{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75651bb6-36d7-4f65-993d-f84e1e5e04f9",
   "metadata": {},
   "source": [
    "# Lesson 5: Python on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bedc1-6531-45bd-be12-6af48b13fdac",
   "metadata": {},
   "source": [
    "Our introduction to array-oriented programming started by addressing Python's slowness (or the slowness of interactive environments in general). But GPUs are also best addressed in an array-at-a-time fashion: the memory layout explicitly favors arrays over objects addressed by pointers or references (even in C++). It shouldn't be a surprise, then, that Python's GPU libraries are array libraries.\n",
    "\n",
    "Most users of GPUs in Python are using them for Machine Learning (ML). Since ML libraries are focused on ML, rather than programming in general, they provide an interface that is identical for CPUs and GPUs, usually by a `device` flag in the array object. (Array objects in ML libraries are called \"tensors\"... for no good reason.) Switching from `device=\"cpu\"` to `device=\"cuda\"` simply speeds up the calculation by an order of magnitude with no other changes.\n",
    "\n",
    "In this lesson, we'll focus on GPU programming in which the differences from CPU programming are visible.\n",
    "\n",
    "The libraries specifically address CUDA, Nvidia's brand of GPU programming, which reflects the very tight monopoly Nvidia has on GPUs for general-purpose programming. ([CuPy has _experimental_ support for AMD](https://docs.cupy.dev/en/stable/install.html#using-cupy-on-amd-gpu-experimental) but [Numba deprecated support for AMD](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021) due to lack of community interest. There are Python projects that support [OpenCL](https://documen.tician.de/pyopencl/), [Vulkan](https://github.com/realitix/vulkan), [Kokkos](https://github.com/kokkos/pykokkos), and [SYCL](https://github.com/IntelPython/dpctl).) Unfortunately, this excludes many laptops, including all MacOS: CUDA is something you'll more likely use on a data center than a laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14affb9-a7cd-4c1c-a567-19bd54851085",
   "metadata": {},
   "source": [
    "## Hints for installing CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e1bd1-630a-4fcf-8936-27f12e0af72f",
   "metadata": {},
   "source": [
    "Installing a working version of CUDA can be hard, as it depends strongly on the operating system and GPU hardware. However, here are some hints:\n",
    "\n",
    "1. The GPU driver must be installed in the operating system (Windows or Linux) and not with any Python package installers (pip, conda, uv, pixi). You might get it directly from Nvidia or through the operating system's package installer (such as apt for Ubuntu).\n",
    "2. The CUDA compiler and its libraries are distinct from the device driver, and they can be installed with conda. The story is more complicated for CUDA 11, but for CUDA 12, just include `cuda-version=12` in the list of packages to install.\n",
    "3. Nvidia publishes a [table of version compatibility](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html) between GPU hardware, drivers, and CUDA packages.\n",
    "4. Numba comes with a `numba -s` commandline tool to print the versions of everything: incompatibilities between driver and CUDA package can be found here. `nvidia-smi` can also be useful.\n",
    "\n",
    "In this lesson, we'll be using the `cupy` and `numba` packages (same names in pip and conda), which will pull all of the CUDA dependencies they need. So if you have an Nvidia GPU and appropriate drivers installed, this should work:\n",
    "\n",
    "```bash\n",
    "conda install cuda-version=12 cupy numba\n",
    "```\n",
    "\n",
    "and this acts as a test that it worked (the random numbers will differ in each run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7317a41-fab9-4f23-893d-63867fd53c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af01590b-46f5-4abb-aea6-201fd54562bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27293273, -1.77580853,  1.93887115, ..., -0.99303592,\n",
       "       -0.95136004, -0.38808653])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cp.random.normal(0, 1, 5 * 1024)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12290b0-9aa4-4947-851b-55f6ed5f5539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31.57886369, -17.67163523,  28.8791174 ,  -6.20866959,\n",
       "       -15.55492089])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.sum(a.reshape(5, 1024), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a46b684-f205-4f1a-98a5-6d83f83cd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numba.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fa7781-1cea-4b04-a0f7-7b49dffb39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.cuda.jit\n",
    "def zero_out(array):\n",
    "    i = nb.cuda.grid(1)\n",
    "    array[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49b0480-f259-424c-95c0-64bb63cfda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_out[1024, 5](a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d252f153-6946-4bdf-8056-b5fcf04ab74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480ef15-d9ea-46ef-96f9-8a571eb6241f",
   "metadata": {},
   "source": [
    "## Brief introduction to GPU programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1731f-02b6-457d-8649-cda9dbf4bc6e",
   "metadata": {},
   "source": [
    "A GPU is a device attached to a computer (usually separate from the main motherboard, through a PCI bus) that has its own memory and can perform calculations.\n",
    "\n",
    "![](img/cpu-vs-gpu.svg){. width=\"100%\"}\n",
    "\n",
    "It differs from the CPUs in several ways. Whereas a typical computer has several CPUs that can all perform calculations independently in parallel, and has a hardware-managed CPU cache to speed up access to recently used data,\n",
    "\n",
    "* a GPU consists of _thousands_ of independent processing units that can work in parallel,\n",
    "* with a much slower clock rate than a CPU,\n",
    "* small groups (32 or 64) of processing units aren't independent; they have to perform the same instructions, but they can apply them to different data,\n",
    "* memory within the GPU is managed manually by the programmer.\n",
    "\n",
    "In Seymour Cray's metaphor, \"If you were plowing a field, which would you rather use: two strong oxen or 1024 chickens?\" a GPU is the thousand chickens. Certain types of problems are faster and more energy efficient to solve with a thousand chickens.\n",
    "\n",
    "An algorithm intended for a CPU, like this:\n",
    "\n",
    "```cuda\n",
    "void some_function(float* array, int index) {\n",
    "    array[index] = ...;\n",
    "}\n",
    "\n",
    "for (int i = 0;  i < 1000000;  i++) {\n",
    "    some_function(array, i);\n",
    "}\n",
    "```\n",
    "\n",
    "would get translated into something like this for a GPU:\n",
    "\n",
    "```cuda\n",
    "__global__ some_function(float* array) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    array[index] = ...;\n",
    "}\n",
    "\n",
    "some_function<<<blocks_per_grid, threads_per_block>>>(array);\n",
    "```\n",
    "\n",
    "The explicit iteration over elements of an `array` (in sequential order) on the CPU is replaced by a GPU \"kernel\" function that applies to just one array element, and each kernel-execution \"thread\" might run at any time. The execution is organized into \"blocks\": threads within a block run at the same time on shared resources, and the GPU schedules block execution in a way that keeps its streaming multiprocessors busy. The `array` must already be in the GPU's global memory.\n",
    "\n",
    "![](img/blocks-in-progress.svg){. width=\"80%\"}\n",
    "\n",
    "The `blocks_per_grid` and `threads_per_block` are units of work—they control how computations are scheduled on the GPU—not units of array allocation. However, for 1-dimensional data, it's often best to define them in terms of the array size:\n",
    "\n",
    "```cuda\n",
    "threads_per_block = 1024;   // maximum possible on most GPUs\n",
    "blocks_per_grid = int(ceil(1.0 * size_of_array / threads_per_block));\n",
    "```\n",
    "\n",
    "This minimizes the number of blocks needed to make sure that each array element is updated by exactly one thread.\n",
    "\n",
    "![](img/blocks-of-array.svg){. width=\"70%\"}\n",
    "\n",
    "If the `size_of_array` doesn't fit neatly into an integer number of blocks, a few threads will be wasted. To make sure that they don't update uninitialized data, you usually need to check for an excess in the kernel function:\n",
    "\n",
    "```cuda\n",
    "__global__ some_function(float* array) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (index < size_of_array) {\n",
    "        array[index] = ...;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We'll see this with explicit examples in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990087b-7a25-4ab9-94bd-85f218f3894f",
   "metadata": {},
   "source": [
    "## CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470859fa-88ff-4c8c-a337-4844b5e93729",
   "metadata": {},
   "source": [
    "[CuPy](https://cupy.dev/) is a library with _mostly_ the same interface as NumPy, but all arrays are in GPU memory, rather than the motherboard's RAM. Converting a NumPy array into a CuPy array, and vice-versa, copies data from RAM to GPU global memory and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec1eae3-dae9-46bc-84a4-8cac6e764753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5a0255-e7d3-46f4-82d9-f98054065a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_in_ram = np.random.uniform(0, 1, 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e294f265-fac2-4e55-b975-32eb535eaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_on_gpu = cp.asarray(array_in_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca9b1bf-ba9c-405a-be4e-39b9d276a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.9 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "array_in_ram[:] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "463c6f4c-446b-4021-a871-96b8dbbd9eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "array_on_gpu[:] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd1c53-c7ea-4e2a-8fe5-e927102f5960",
   "metadata": {},
   "source": [
    "It's important that the distinction is visible like this, since copying an array between RAM and the GPU is often more time-consuming than the computation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55199a7-e664-415c-a432-a3df9f84a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "cp.asarray(array_in_ram)   # from RAM to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55174fd-59ce-4e95-8c98-1ca26afcc40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "array_on_gpu.get()         # from GPU to RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6f3e9-3332-4459-b879-8104866f8328",
   "metadata": {},
   "source": [
    "Thus, you'll want to keep the data on the device that is performing calculations for as long as possible. If data can be created on the GPU, such as random numbers for a Monte Carlo calculation, use CuPy's functions to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0162b39-eea7-430e-9eba-1d41f8bcb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_on_gpu = cp.random.uniform(0, 1, 100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042cd99-7838-477e-9401-531982aaea71",
   "metadata": {},
   "source": [
    "We can see more of what's going on by running Nvidia's `nsys-ui` profiler on the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92cbaf4-1d18-4716-ac69-cb0038a20991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.        , 5.00000007, 5.0000002 , ..., 9.99999962, 9.99999966,\n",
       "       9.9999998 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = cp.random.uniform(5, 10, 100000000)\n",
    "array.sort()\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491eb34-317b-41d1-b672-a7ff70edd76e",
   "metadata": {},
   "source": [
    "![](img/nsys-profiler-1.png){. width=\"100%\"}\n",
    "\n",
    "The CUDA kernels (blue) from the first line runs\n",
    "\n",
    "* `cupy_random_x_mod_1`\n",
    "* `cupy_scale`\n",
    "\n",
    "to make the random numbers, followed by\n",
    "\n",
    "* `DeviceRadixSortHistogramKernel`\n",
    "* `DeviceRadixSortOnesweepKernel`\n",
    "\n",
    "to sort the random numbers. Meanwhile on the CPU (green), Python waits for the result with a\n",
    "\n",
    "* `cudaStreamSynchronize`\n",
    "\n",
    "Since the GPU is a separate device from the CPU, it runs independently. If you don't ask for the result, the Python function can finish while the GPU calculation is still running. It's only when you want to print the result, copy it to a NumPy array, or get any element as a Python number that CuPy asks the GPU to \"synchronize\"—wait for computation to be finished.\n",
    "\n",
    "Since the GPU runs independently, what should it do if it encounters an error part-way through processing? It can't raise a Python error (the Python function call has already finished), so it has to do something else instead. CuPy defines some results that would be errors in NumPy. For instance, this array-slice asks for elements beyond the end of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9cdce04-5ecb-482e-b773-aa045350349a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m2.2\u001b[39m, \u001b[38;5;241m3.3\u001b[39m, \u001b[38;5;241m4.4\u001b[39m, \u001b[38;5;241m5.5\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "array = np.array([0.0, 1.1, 2.2, 3.3, 4.4, 5.5])\n",
    "\n",
    "array[np.array([2, 3, 5, 6, 7, 8])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbce4b4-1937-4f0e-b2d6-23f3b07f70ca",
   "metadata": {},
   "source": [
    "But CuPy returns a result by wrapping the indexes around to the beginning of the array (6 → 0, 7 → 1, 8 → 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f368209-743b-461b-be5f-2e0bdf0c7950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2, 3.3, 5.5, 0. , 1.1, 2.2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = cp.array([0.0, 1.1, 2.2, 3.3, 4.4, 5.5])\n",
    "\n",
    "array[cp.array([2, 3, 5, 6, 7, 8])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c9f19-90bb-4994-9475-023e8fc9456d",
   "metadata": {},
   "source": [
    "So you may need to check the correctness of your code in NumPy before running it at high speed in CuPy.\n",
    "\n",
    "Next, we should note that just replacing NumPy with CuPy has the same shortcoming as array-oriented Python versus compiled code: although it's a first (easy) step toward speeding up a computation, it's not the fastest possible because intermediate arrays have to be allocated in each step.\n",
    "\n",
    "Remember how the quadratic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a8473ac-8484-4952-a0b5-72a949ffd75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00354629,  0.00108686, -0.0014943 , ...,  0.00375009,\n",
       "        0.00123045, -0.00259124])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cp.random.uniform(5, 10, 1000000)\n",
    "b = cp.random.uniform(10, 20, 1000000)\n",
    "c = cp.random.uniform(-0.1, 0.1, 1000000)\n",
    "\n",
    "(-b + cp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541d227-e702-48d5-b1d1-314fece076ee",
   "metadata": {},
   "source": [
    "is roughly equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc324eb-301a-4bd7-b581-978ddb6e668f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00354629,  0.00108686, -0.0014943 , ...,  0.00375009,\n",
       "        0.00123045, -0.00259124])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1 = cp.negative(b)            # -b\n",
    "tmp2 = cp.square(b)              # b**2\n",
    "tmp3 = cp.multiply(4, a)         # 4*a\n",
    "tmp4 = cp.multiply(tmp3, c)      # tmp3*c\n",
    "del tmp3\n",
    "tmp5 = cp.subtract(tmp2, tmp4)   # tmp2 - tmp4\n",
    "del tmp2, tmp4\n",
    "tmp6 = cp.sqrt(tmp5)             # sqrt(tmp5)\n",
    "del tmp5\n",
    "tmp7 = cp.add(tmp1, tmp6)        # tmp1 + tmp6\n",
    "del tmp1, tmp6\n",
    "tmp8 = cp.multiply(2, a)         # 2*a\n",
    "np.divide(tmp7, tmp8)            # tmp7 / tmp8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62791140-a508-4bf4-8e68-c1ac0a5eb7e7",
   "metadata": {},
   "source": [
    "Here's what that looks like in the profiler:\n",
    "\n",
    "![](img/nsys-profiler-2.png){. width=\"100%\"}\n",
    "\n",
    "Each of the\n",
    "\n",
    "* `cupy_power__float64_float_float64`\n",
    "* `cupy_multiply__float_float64_float64`\n",
    "* `cupy_sqrt__float64_float64`\n",
    "* `cupy_true_divide__float64_float64_float64`\n",
    "* `cupy_multiply__float64_float64_float64`\n",
    "* `cupy_subtract__float64_float64_float64`\n",
    "* `cupy_add__float64_float64_float64`\n",
    "* `cupy_negative__float64_float64`\n",
    "\n",
    "kernels runs quickly, but there are time-gaps between them in which arrays are allocated and dynamic code decides what to do next. It would be faster if the whole quadratic formula were \"fused\" into a single kernel.\n",
    "\n",
    "To support this, CuPy has a JIT compiler that lets you write C++ CUDA code. For instance, the worst part of the calculation above is using a general floating-point `power` function,\n",
    "\n",
    "* `cupy_power__float64_float_float64`\n",
    "\n",
    "to compute `power(b, 2)`, which should be just `b * b`. Let's define a better function for \"raising to an integer power\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b616b474-ecf5-4a44-9e4b-055ebe9a02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cupy._core._kernel.ElementwiseKernel at 0x7228e4267060>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intpow = cp.ElementwiseKernel(\"float64 x, int64 n\", \"float64 out\", '''\n",
    "    out = 1.0;\n",
    "    for (int i = 0;  i < n;  i++) {\n",
    "        out *= x;\n",
    "    }\n",
    "''', \"intpow\")\n",
    "intpow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a05fc2-3b6e-4e44-ac69-999539a25815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([349.69803979, 119.27597606, 300.9489732 , ..., 227.78363405,\n",
       "       235.76110542, 178.84017007])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intpow(b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20568fe-fa91-480e-9870-77b7505d1bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([349.69803979, 119.27597606, 300.9489732 , ..., 227.78363405,\n",
       "       235.76110542, 178.84017007])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b1820-14c4-4e9b-b968-edfadb6b1de3",
   "metadata": {},
   "source": [
    "We can also do this with the whole quadratic formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b49a3004-9b9b-443d-a0cf-b8cf1c7a5f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00354629,  0.00108686, -0.0014943 , ...,  0.00375009,\n",
       "        0.00123045, -0.00259124])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_formula = cp.ElementwiseKernel(\"float64 a, float64 b, float64 c\", \"float64 out\", '''\n",
    "    out = (-b + sqrt(b*b - 4*a*c)) / (2*a);\n",
    "''', \"quadratic_formula\")\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94378943-eab0-4256-9285-945d5cccd2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n1000\n",
    "\n",
    "(-b + cp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16613ff7-5372-4ca3-a7c5-421c50b966c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 μs ± 0 ns per loop (mean ± std. dev. of 1 run, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n1000\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c116327-aeed-49ab-9499-5cf695c5ccc6",
   "metadata": {},
   "source": [
    "Both of the above used [ElementwiseKernel](https://docs.cupy.dev/en/stable/user_guide/kernel.html), which is like a NumPy ufunc: they take arrays and apply the function to each element. We could also write a fully generic [RawKernel](https://docs.cupy.dev/en/stable/user_guide/kernel.html#raw-kernels), but then we'd have to manage the `threads_per_block` and `blocks_per_grid` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef08c8f4-9b4f-488e-9ff1-932c54e612a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00354629,  0.00108686, -0.0014943 , ...,  0.00375009,\n",
       "        0.00123045, -0.00259124])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_formula_raw = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void quadratic_formula_raw(const double* a, const double* b, const double* c, int length, double* out) {\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < length) {\n",
    "        out[i] = (-b[i] + sqrt(b[i]*b[i] - 4*a[i]*c[i])) / (2*a[i]);\n",
    "    }\n",
    "}\n",
    "''', \"quadratic_formula_raw\")\n",
    "\n",
    "out = cp.empty_like(a)\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(len(out) / 1024))\n",
    "\n",
    "quadratic_formula_raw((blocks_per_grid,), (threads_per_block,), (a, b, c, len(out), out))\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011ae3e-86b0-46eb-9c48-3926b01ddaf4",
   "metadata": {},
   "source": [
    "Thus, CuPy is two things:\n",
    "\n",
    "* an array library with the NumPy interface, but all arrays are on GPUs,\n",
    "* a JIT compiler of CUDA kernels written in C++.\n",
    "\n",
    "This situation is similar to writing custom C++ code with pybind11, rather than writing the kernel in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01356173-c8dd-454e-afd0-5138ce0c4ee9",
   "metadata": {},
   "source": [
    "## Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477adb5-3554-4112-b410-c3f31a25204a",
   "metadata": {},
   "source": [
    "Just as Numba can JIT compile Python for CPUs, it can [JIT compile Python to CUDA](https://numba.readthedocs.io/en/stable/cuda/index.html).\n",
    "\n",
    "Here's the quadratic formula as a single CUDA kernel without writing any C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a55a531c-a66a-45b0-a551-5f815691f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda  # must be explicitly imported\n",
    "import math        # note that Numba-CUDA requires math.*; you can't use np.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de305454-fe63-4963-b1db-c2a396567c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00354629,  0.00108686, -0.0014943 , ...,  0.00375009,\n",
       "        0.00123045, -0.00259124])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nb.cuda.jit\n",
    "def quadratic_formula_numba_cuda(a, b, c, out):\n",
    "    i = nb.cuda.grid(1)   # 1-dimensional\n",
    "    if i < len(out):\n",
    "        out[i] = (-b[i] + math.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "\n",
    "out = cp.empty_like(a)\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(len(out) / 1024))\n",
    "\n",
    "quadratic_formula_numba_cuda[blocks_per_grid, threads_per_block](a, b, c, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9225857c-56a8-4d76-b0b3-52816468367b",
   "metadata": {},
   "source": [
    "Here's what it looks like in the profiler:\n",
    "\n",
    "![](img/nsys-profiler-3.png){. width=\"100%\"}\n",
    "\n",
    "The name is a long random string, but it all runs in a short block of time.\n",
    "\n",
    "Note that the structure of a `@nb.cuda.jit` compiled function is equivalent to a C++ CUDA function: all that has changed is the Python syntax and the names of some of the functions:\n",
    "\n",
    "```cuda\n",
    "extern \"C\" __global__\n",
    "void quadratic_formula_raw(const double* a, const double* b, const double* c, int length, double* out) {\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < length) {\n",
    "        out[i] = (-b[i] + sqrt(b[i]*b[i] - 4*a[i]*c[i])) / (2*a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "quadratic_formula_raw<<<blocks_per_grid, threads_per_block>>>(a, b, c, size_of_array, out);\n",
    "```\n",
    "\n",
    "versus\n",
    "\n",
    "```python\n",
    "@nb.cuda.jit\n",
    "def quadratic_formula_numba_cuda(a, b, c, out):\n",
    "    i = nb.cuda.grid(1)   # 1-dimensional\n",
    "    if i < len(out):\n",
    "        out[i] = (-b[i] + math.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "\n",
    "quadratic_formula_numba_cuda[blocks_per_grid, threads_per_block](a, b, c, out)\n",
    "```\n",
    "\n",
    "You still have to choose an optimal `blocks_per_grid` and `threads_per_block`, and the kernel gets its thread index through a special function, [nb.cuda.grid](https://numba.readthedocs.io/en/stable/cuda-reference/kernel.html#numba.cuda.grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b737ac-c8cc-4ef8-890a-00eb030a9a90",
   "metadata": {},
   "source": [
    "## JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac01a2-3d9b-4bb2-bb36-83d5c21b7c6c",
   "metadata": {},
   "source": [
    "Just as JAX inspects array-oriented code, identifies which functions can be fused, and JIT compiles the fused kernel for the CPU, it can do the same for GPUs. In fact, a JAX array has a `device` parameter that determines whether it will be in RAM or on the GPU.\n",
    "\n",
    "Instead of performing the same demonstration with the quadratic formula, see `mandelbrot-on-all-accelerators.ipynb` in your `notebooks` folder. It takes a single calculation, which draws the Mandelbrot set, and accelerates it using all of the techniques we've discussed, including CuPy's RawKernel, Numba-CUDA, and JAX-CUDA. Here's the bottom line:\n",
    "\n",
    "![](img/plot-mandelbrot-on-all-accelerators.svg){. width=\"100%\"}\n",
    "\n",
    "Custom CUDA kernels are the fastest, regardless of whether they are compiled by CuPy, Numba, or JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66580f6d-050e-42d1-b02d-61b498a2ba74",
   "metadata": {},
   "source": [
    "## Awkward Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c839b6-530d-4439-a4ae-a325229e7ce6",
   "metadata": {},
   "source": [
    "Awkward Arrays can be copied to a GPU using [ak.to_backend](https://awkward-array.org/doc/main/reference/generated/ak.to_backend.html) with `backend=\"cuda\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d66e80b-4956-4cd9-9751-587e2465ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bde3cb25-4fe9-4d88-8dc5-475845cb6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open(\"data/SMHiggsToZZTo4L.root:Events\") as tree:\n",
    "    events_pt, events_eta, events_phi, events_charge = tree.arrays(\n",
    "        [\"Electron_pt\", \"Electron_eta\", \"Electron_phi\", \"Electron_charge\"], how=tuple\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5612e4ea-5521-46af-904c-bdb07581ccf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[[],\n",
       " [{pt: 21.90268, eta: -0.7021887, phi: 0.13399617, charge: 1}, ..., {...}],\n",
       " [{pt: 11.571167, eta: -0.8493275, phi: 2.1804526, charge: -1}, {...}],\n",
       " [{pt: 10.413637, eta: -0.022762615, phi: -0.64317286, charge: -1}],\n",
       " [{pt: 30.733414, eta: -0.610117, phi: 1.0087388, charge: -1}, ..., {...}],\n",
       " [{pt: 16.080765, eta: 0.34038106, phi: -0.4972907, charge: 1}],\n",
       " [{pt: 5.3183117, eta: -0.5736427, phi: -1.4705021, charge: -1}],\n",
       " [{pt: 6.988549, eta: 0.52973014, phi: -0.6329002, charge: 1}],\n",
       " [{pt: 40.959602, eta: -0.21598065, phi: 2.7890391, charge: -1}, ..., {...}],\n",
       " [],\n",
       " ...,\n",
       " [{pt: 19.12917, eta: 1.1267458, phi: -0.4354926, charge: -1}, {...}],\n",
       " [{pt: 30.197298, eta: 1.8354917, phi: 1.9399015, charge: 1}],\n",
       " [],\n",
       " [{pt: 37.00934, eta: -2.3536925, phi: 0.9030401, charge: 1}, ..., {...}],\n",
       " [],\n",
       " [{pt: 12.313824, eta: -0.27283365, phi: -3.108863, charge: 1}],\n",
       " [],\n",
       " [{pt: 17.923506, eta: 2.2025933, phi: -1.1234515, charge: -1}, {...}],\n",
       " [{pt: 48.07147, eta: 0.53210974, phi: -1.6076394, charge: -1}, {...}]]\n",
       "---------------------------------------------------------------------------------------------------------\n",
       "backend: cuda\n",
       "nbytes: 9.2 MB\n",
       "type: 299973 * var * Momentum4D[\n",
       "    pt: float32,\n",
       "    eta: float32,\n",
       "    phi: float32,\n",
       "    charge: int32\n",
       "]</pre>"
      ],
      "text/plain": [
       "<Array [[], ..., [{pt: 48.07147, ...}, ...]] type='299973 * var * Momentum4...'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electrons = ak.to_backend(\n",
    "    ak.zip({\n",
    "        \"pt\": events_pt,\n",
    "        \"eta\": events_eta,\n",
    "        \"phi\": events_phi,\n",
    "        \"charge\": events_charge,\n",
    "    },\n",
    "    with_name=\"Momentum4D\",\n",
    "), \"cuda\")\n",
    "\n",
    "electrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada706a-dea6-43ee-b82d-ee3743e80aaf",
   "metadata": {},
   "source": [
    "With this backend, all mathematical computations are performed by CuPy (directly or through JIT compiled functions).\n",
    "\n",
    "Below is a calculation of the mass of the heaviest dielectron in each event, using\n",
    "\n",
    "$$\\sqrt{2\\,{p_T}_1\\,{p_T}_2\\left(\\cosh(\\eta_1 - \\eta_2) - \\cos(\\phi_1 - \\phi_2)\\right)}$$\n",
    "\n",
    "for the invariant mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ba1ab6a-733d-4bae-8a92-57c5b98b744c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[None,\n",
       " 76.752525,\n",
       " 36.007294,\n",
       " None,\n",
       " 87.34057,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 92.22189,\n",
       " None,\n",
       " ...,\n",
       " 27.277554,\n",
       " None,\n",
       " None,\n",
       " 107.22261,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 40.64742,\n",
       " 91.51784]\n",
       "-----------------------\n",
       "backend: cuda\n",
       "nbytes: 1.5 MB\n",
       "type: 299973 * ?float32</pre>"
      ],
      "text/plain": [
       "<Array [None, 76.752525, ..., 40.64742, 91.51784] type='299973 * ?float32'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1, e2 = ak.unzip(ak.combinations(electrons, 2))\n",
    "z_mass = np.sqrt(\n",
    "    2*e1.pt*e2.pt * (np.cosh(e1.eta - e2.eta) - np.cos(e1.phi - e2.phi))\n",
    ")\n",
    "np.max(z_mass, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60979df-226d-420f-a608-9baf536acafd",
   "metadata": {},
   "source": [
    "GPU-bound Awkward Arrays can also be iterated over in Numba-CUDA, which allows for imperative code. However, the output must be a non-ragged CuPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "648cbbb1-1900-47f9-98e1-c791eac4876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.      , 76.752525, 36.007294, ...,  0.      , 40.647415,\n",
       "       91.51784 ], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.numba.register_and_check()\n",
    "\n",
    "@nb.cuda.jit(extensions=[ak.numba.cuda])\n",
    "def mass_of_heaviest_dielectron(electrons, out):\n",
    "    thread_idx = nb.cuda.grid(1)\n",
    "    if thread_idx < len(electrons):\n",
    "        electrons_in_one_event = electrons[thread_idx]\n",
    "        for i, e1 in enumerate(electrons_in_one_event):\n",
    "            for e2 in electrons_in_one_event[i + 1:]:\n",
    "                if e1.charge != e2.charge:\n",
    "                    m = math.sqrt(\n",
    "                        2*e1.pt*e2.pt * (math.cosh(e1.eta - e2.eta) - math.cos(e1.phi - e2.phi))\n",
    "                    )\n",
    "                    if m > out[thread_idx]:\n",
    "                        out[thread_idx] = m\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(len(electrons) / 1024))\n",
    "\n",
    "out = cp.zeros(len(electrons), dtype=np.float32)\n",
    "mass_of_heaviest_dielectron[blocks_per_grid, threads_per_block](electrons, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5792b91-638d-43a7-969d-a74a74776a91",
   "metadata": {},
   "source": [
    "To make this a little more readable, let's rewrite it as a `__device__` function, which is a function that has a return value (doesn't have to overwrite an array) but can only be called from GPU-bound functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0ff2d11-0759-452c-9b7b-9512f99760da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.      , 76.752525, 36.007294, ...,  0.      , 40.647415,\n",
       "       91.51784 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nb.cuda.jit(extensions=[ak.numba.cuda], device=True)\n",
    "def compute_mass(event):\n",
    "    out = np.float32(0)\n",
    "    for i, e1 in enumerate(event):\n",
    "        for e2 in event[i + 1:]:\n",
    "            if e1.charge != e2.charge:\n",
    "                m = math.sqrt(\n",
    "                    2*e1.pt*e2.pt * (math.cosh(e1.eta - e2.eta) - math.cos(e1.phi - e2.phi))\n",
    "                )\n",
    "                if m > out:\n",
    "                    out = m\n",
    "    return out\n",
    "\n",
    "@nb.cuda.jit(extensions=[ak.numba.cuda])\n",
    "def mass_of_heaviest_dielectron_2(events, out):\n",
    "    thread_idx = nb.cuda.grid(1)\n",
    "    if thread_idx < len(events):\n",
    "        out[thread_idx] = compute_mass(events[thread_idx])\n",
    "\n",
    "# same threads_per_block, blocks_per_grid\n",
    "\n",
    "out = cp.zeros(len(electrons), dtype=np.float32)\n",
    "mass_of_heaviest_dielectron_2[blocks_per_grid, threads_per_block](electrons, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e6493-d6fe-49e4-b8b0-9f9928dcf457",
   "metadata": {},
   "source": [
    "## Lesson 5 project: Histograms and Monte Carlo on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2ea66-a616-4b20-a0e5-75aac39c90e5",
   "metadata": {},
   "source": [
    "As described in the [intro](0-intro.md), navigate to the `notebooks` directory and open `lesson-5-project.ipynb`, then follow its instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
