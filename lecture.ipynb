{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75651bb6-36d7-4f65-993d-f84e1e5e04f9",
   "metadata": {},
   "source": [
    "# Lesson 5: Python on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561f5e4-29ac-4436-80f2-d973cc1c2c6a",
   "metadata": {},
   "source": [
    "Lets see what sort of GPU we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8497ea0-4388-480e-bd2a-8531141dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c8d5f-871b-41ea-aee2-9624c3348b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c088c-d05e-4655-bba7-1adaf7b0f165",
   "metadata": {},
   "source": [
    "Ok - we have a GPU and it looks to be setup correctly. Lets try to use the CUPY library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7317a41-fab9-4f23-893d-63867fd53c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01590b-46f5-4abb-aea6-201fd54562bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cp.random.normal(0, 1, 5 * 1024)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12290b0-9aa4-4947-851b-55f6ed5f5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.sum(a.reshape(5, 1024), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71295975-f434-4994-ba0c-781c3d8c83e3",
   "metadata": {},
   "source": [
    "And Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46b684-f205-4f1a-98a5-6d83f83cd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numba.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa7781-1cea-4b04-a0f7-7b49dffb39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.cuda.jit\n",
    "def zero_out(array):\n",
    "    i = nb.cuda.grid(1)\n",
    "    array[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b0480-f259-424c-95c0-64bb63cfda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_out[1024, 5](a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252f153-6946-4bdf-8056-b5fcf04ab74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990087b-7a25-4ab9-94bd-85f218f3894f",
   "metadata": {},
   "source": [
    "## CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470859fa-88ff-4c8c-a337-4844b5e93729",
   "metadata": {},
   "source": [
    "[CuPy](https://cupy.dev/) is a library with _mostly_ the same interface as NumPy, but all arrays are in GPU memory, rather than the motherboard's RAM. Converting a NumPy array into a CuPy array, and vice-versa, copies data from RAM to GPU global memory and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1eae3-dae9-46bc-84a4-8cac6e764753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a0255-e7d3-46f4-82d9-f98054065a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_in_ram = np.random.uniform(0, 1, 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294f265-fac2-4e55-b975-32eb535eaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_on_gpu = cp.asarray(array_in_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9b1bf-ba9c-405a-be4e-39b9d276a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "array_in_ram[:] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c6f4c-446b-4021-a871-96b8dbbd9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "array_on_gpu[:] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd1c53-c7ea-4e2a-8fe5-e927102f5960",
   "metadata": {},
   "source": [
    "It's important that the distinction is visible like this, since copying an array between RAM and the GPU is often more time-consuming than the computation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55199a7-e664-415c-a432-a3df9f84a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "cp.asarray(array_in_ram)   # from RAM to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55174fd-59ce-4e95-8c98-1ca26afcc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r1 -n10\n",
    "\n",
    "array_on_gpu.get()         # from GPU to RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6f3e9-3332-4459-b879-8104866f8328",
   "metadata": {},
   "source": [
    "Thus, you'll want to keep the data on the device that is performing calculations for as long as possible. If data can be created on the GPU, such as random numbers for a Monte Carlo calculation, use CuPy's functions to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0162b39-eea7-430e-9eba-1d41f8bcb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_on_gpu = cp.random.uniform(0, 1, 100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042cd99-7838-477e-9401-531982aaea71",
   "metadata": {},
   "source": [
    "We can see more of what's going on by running Nvidia's `nsys-ui` profiler on the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92cbaf4-1d18-4716-ac69-cb0038a20991",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = cp.random.uniform(5, 10, 100000000)\n",
    "array.sort()\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491eb34-317b-41d1-b672-a7ff70edd76e",
   "metadata": {},
   "source": [
    "![](img/nsys-profiler-1.png){. width=\"100%\"}\n",
    "\n",
    "The CUDA kernels (blue) from the first line runs\n",
    "\n",
    "* `cupy_random_x_mod_1`\n",
    "* `cupy_scale`\n",
    "\n",
    "to make the random numbers, followed by\n",
    "\n",
    "* `DeviceRadixSortHistogramKernel`\n",
    "* `DeviceRadixSortOnesweepKernel`\n",
    "\n",
    "to sort the random numbers. Meanwhile on the CPU (green), Python waits for the result with a\n",
    "\n",
    "* `cudaStreamSynchronize`\n",
    "\n",
    "Since the GPU is a separate device from the CPU, it runs independently. If you don't ask for the result, the Python function can finish while the GPU calculation is still running. It's only when you want to print the result, copy it to a NumPy array, or get any element as a Python number that CuPy asks the GPU to \"synchronize\"—wait for computation to be finished.\n",
    "\n",
    "Since the GPU runs independently, what should it do if it encounters an error part-way through processing? It can't raise a Python error (the Python function call has already finished), so it has to do something else instead. CuPy defines some results that would be errors in NumPy. For instance, this array-slice asks for elements beyond the end of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdce04-5ecb-482e-b773-aa045350349a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "array = np.array([0.0, 1.1, 2.2, 3.3, 4.4, 5.5])\n",
    "\n",
    "array[np.array([2, 3, 5, 6, 7, 8])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbce4b4-1937-4f0e-b2d6-23f3b07f70ca",
   "metadata": {},
   "source": [
    "But CuPy returns a result by wrapping the indexes around to the beginning of the array (6 → 0, 7 → 1, 8 → 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f368209-743b-461b-be5f-2e0bdf0c7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = cp.array([0.0, 1.1, 2.2, 3.3, 4.4, 5.5])\n",
    "\n",
    "array[cp.array([2, 3, 5, 6, 7, 8])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c9f19-90bb-4994-9475-023e8fc9456d",
   "metadata": {},
   "source": [
    "So you may need to check the correctness of your code in NumPy before running it at high speed in CuPy.\n",
    "\n",
    "Next, we should note that just replacing NumPy with CuPy has the same shortcoming as array-oriented Python versus compiled code: although it's a first (easy) step toward speeding up a computation, it's not the fastest possible because intermediate arrays have to be allocated in each step.\n",
    "\n",
    "Remember how the quadratic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8473ac-8484-4952-a0b5-72a949ffd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cp.random.uniform(5, 10, 1000000)\n",
    "b = cp.random.uniform(10, 20, 1000000)\n",
    "c = cp.random.uniform(-0.1, 0.1, 1000000)\n",
    "\n",
    "(-b + cp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541d227-e702-48d5-b1d1-314fece076ee",
   "metadata": {},
   "source": [
    "is roughly equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc324eb-301a-4bd7-b581-978ddb6e668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = cp.negative(b)            # -b\n",
    "tmp2 = cp.square(b)              # b**2\n",
    "tmp3 = cp.multiply(4, a)         # 4*a\n",
    "tmp4 = cp.multiply(tmp3, c)      # tmp3*c\n",
    "del tmp3\n",
    "tmp5 = cp.subtract(tmp2, tmp4)   # tmp2 - tmp4\n",
    "del tmp2, tmp4\n",
    "tmp6 = cp.sqrt(tmp5)             # sqrt(tmp5)\n",
    "del tmp5\n",
    "tmp7 = cp.add(tmp1, tmp6)        # tmp1 + tmp6\n",
    "del tmp1, tmp6\n",
    "tmp8 = cp.multiply(2, a)         # 2*a\n",
    "np.divide(tmp7, tmp8)            # tmp7 / tmp8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62791140-a508-4bf4-8e68-c1ac0a5eb7e7",
   "metadata": {},
   "source": [
    "Here's what that looks like in the profiler:\n",
    "\n",
    "![](img/nsys-profiler-2.png){. width=\"100%\"}\n",
    "\n",
    "Each of the\n",
    "\n",
    "* `cupy_power__float64_float_float64`\n",
    "* `cupy_multiply__float_float64_float64`\n",
    "* `cupy_sqrt__float64_float64`\n",
    "* `cupy_true_divide__float64_float64_float64`\n",
    "* `cupy_multiply__float64_float64_float64`\n",
    "* `cupy_subtract__float64_float64_float64`\n",
    "* `cupy_add__float64_float64_float64`\n",
    "* `cupy_negative__float64_float64`\n",
    "\n",
    "kernels runs quickly, but there are time-gaps between them in which arrays are allocated and dynamic code decides what to do next. It would be faster if the whole quadratic formula were \"fused\" into a single kernel.\n",
    "\n",
    "To support this, CuPy has a JIT compiler that lets you write C++ CUDA code. For instance, the worst part of the calculation above is using a general floating-point `power` function,\n",
    "\n",
    "* `cupy_power__float64_float_float64`\n",
    "\n",
    "to compute `power(b, 2)`, which should be just `b * b`. Let's define a better function for \"raising to an integer power\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616b474-ecf5-4a44-9e4b-055ebe9a02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intpow = cp.ElementwiseKernel(\"float64 x, int64 n\", \"float64 out\", '''\n",
    "    out = 1.0;\n",
    "    for (int i = 0;  i < n;  i++) {\n",
    "        out *= x;\n",
    "    }\n",
    "''', \"intpow\")\n",
    "intpow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a05fc2-3b6e-4e44-ac69-999539a25815",
   "metadata": {},
   "outputs": [],
   "source": [
    "intpow(b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20568fe-fa91-480e-9870-77b7505d1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b1820-14c4-4e9b-b968-edfadb6b1de3",
   "metadata": {},
   "source": [
    "We can also do this with the whole quadratic formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a3004-9b9b-443d-a0cf-b8cf1c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_formula = cp.ElementwiseKernel(\"float64 a, float64 b, float64 c\", \"float64 out\", '''\n",
    "    out = (-b + sqrt(b*b - 4*a*c)) / (2*a);\n",
    "''', \"quadratic_formula\")\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94378943-eab0-4256-9285-945d5cccd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1000\n",
    "\n",
    "(-b + cp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16613ff7-5372-4ca3-a7c5-421c50b966c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1000\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c116327-aeed-49ab-9499-5cf695c5ccc6",
   "metadata": {},
   "source": [
    "Both of the above used [ElementwiseKernel](https://docs.cupy.dev/en/stable/user_guide/kernel.html), which is like a NumPy ufunc: they take arrays and apply the function to each element. We could also write a fully generic [RawKernel](https://docs.cupy.dev/en/stable/user_guide/kernel.html#raw-kernels), but then we'd have to manage the `threads_per_block` and `blocks_per_grid` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08c8f4-9b4f-488e-9ff1-932c54e612a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_formula_raw = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void quadratic_formula_raw(const double* a, const double* b, const double* c, int length, double* out) {\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < length) {\n",
    "        out[i] = (-b[i] + sqrt(b[i]*b[i] - 4*a[i]*c[i])) / (2*a[i]);\n",
    "    }\n",
    "}\n",
    "''', \"quadratic_formula_raw\")\n",
    "\n",
    "out = cp.empty_like(a)\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(len(out) / 1024))\n",
    "\n",
    "quadratic_formula_raw((blocks_per_grid,), (threads_per_block,), (a, b, c, len(out), out))\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011ae3e-86b0-46eb-9c48-3926b01ddaf4",
   "metadata": {},
   "source": [
    "Thus, CuPy is two things:\n",
    "\n",
    "* an array library with the NumPy interface, but all arrays are on GPUs,\n",
    "* a JIT compiler of CUDA kernels written in C++.\n",
    "\n",
    "This situation is similar to writing custom C++ code with pybind11, rather than writing the kernel in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01356173-c8dd-454e-afd0-5138ce0c4ee9",
   "metadata": {},
   "source": [
    "## Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477adb5-3554-4112-b410-c3f31a25204a",
   "metadata": {},
   "source": [
    "Just as Numba can JIT compile Python for CPUs, it can [JIT compile Python to CUDA](https://numba.readthedocs.io/en/stable/cuda/index.html).\n",
    "\n",
    "Here's the quadratic formula as a single CUDA kernel without writing any C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a531c-a66a-45b0-a551-5f815691f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda  # must be explicitly imported\n",
    "import math        # note that Numba-CUDA requires math.*; you can't use np.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de305454-fe63-4963-b1db-c2a396567c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.cuda.jit\n",
    "def quadratic_formula_numba_cuda(a, b, c, out):\n",
    "    i = nb.cuda.grid(1)   # 1-dimensional\n",
    "    if i < len(out):\n",
    "        out[i] = (-b[i] + math.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "\n",
    "out = cp.empty_like(a)\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(len(out) / 1024))\n",
    "\n",
    "quadratic_formula_numba_cuda[blocks_per_grid, threads_per_block](a, b, c, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9225857c-56a8-4d76-b0b3-52816468367b",
   "metadata": {},
   "source": [
    "Here's what it looks like in the profiler:\n",
    "\n",
    "![](img/nsys-profiler-3.png){. width=\"100%\"}\n",
    "\n",
    "The name is a long random string, but it all runs in a short block of time.\n",
    "\n",
    "Note that the structure of a `@nb.cuda.jit` compiled function is equivalent to a C++ CUDA function: all that has changed is the Python syntax and the names of some of the functions:\n",
    "\n",
    "```cuda\n",
    "extern \"C\" __global__\n",
    "void quadratic_formula_raw(const double* a, const double* b, const double* c, int length, double* out) {\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < length) {\n",
    "        out[i] = (-b[i] + sqrt(b[i]*b[i] - 4*a[i]*c[i])) / (2*a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "quadratic_formula_raw<<<blocks_per_grid, threads_per_block>>>(a, b, c, size_of_array, out);\n",
    "```\n",
    "\n",
    "versus\n",
    "\n",
    "```python\n",
    "@nb.cuda.jit\n",
    "def quadratic_formula_numba_cuda(a, b, c, out):\n",
    "    i = nb.cuda.grid(1)   # 1-dimensional\n",
    "    if i < len(out):\n",
    "        out[i] = (-b[i] + math.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "\n",
    "quadratic_formula_numba_cuda[blocks_per_grid, threads_per_block](a, b, c, out)\n",
    "```\n",
    "\n",
    "You still have to choose an optimal `blocks_per_grid` and `threads_per_block`, and the kernel gets its thread index through a special function, [nb.cuda.grid](https://numba.readthedocs.io/en/stable/cuda-reference/kernel.html#numba.cuda.grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b737ac-c8cc-4ef8-890a-00eb030a9a90",
   "metadata": {},
   "source": [
    "## JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac01a2-3d9b-4bb2-bb36-83d5c21b7c6c",
   "metadata": {},
   "source": [
    "Just as JAX inspects array-oriented code, identifies which functions can be fused, and JIT compiles the fused kernel for the CPU, it can do the same for GPUs. In fact, a JAX array has a `device` parameter that determines whether it will be in RAM or on the GPU.\n",
    "\n",
    "Instead of performing the same demonstration with the quadratic formula, see `mandelbrot-on-all-accelerators.ipynb` in your `notebooks` folder. It takes a single calculation, which draws the Mandelbrot set, and accelerates it using all of the techniques we've discussed, including CuPy's RawKernel, Numba-CUDA, and JAX-CUDA. Here's the bottom line:\n",
    "\n",
    "![](img/plot-mandelbrot-on-all-accelerators.svg){. width=\"100%\"}\n",
    "\n",
    "Custom CUDA kernels are the fastest, regardless of whether they are compiled by CuPy, Numba, or JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66580f6d-050e-42d1-b02d-61b498a2ba74",
   "metadata": {},
   "source": [
    "## Awkward Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c839b6-530d-4439-a4ae-a325229e7ce6",
   "metadata": {},
   "source": [
    "Awkward Arrays can be copied to a GPU using [ak.to_backend](https://awkward-array.org/doc/main/reference/generated/ak.to_backend.html) with `backend=\"cuda\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66e80b-4956-4cd9-9751-587e2465ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3cb25-4fe9-4d88-8dc5-475845cb6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open(\"data/SMHiggsToZZTo4L.root:Events\") as tree:\n",
    "    events_pt, events_eta, events_phi, events_charge = tree.arrays(\n",
    "        [\"Electron_pt\", \"Electron_eta\", \"Electron_phi\", \"Electron_charge\"], how=tuple\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612e4ea-5521-46af-904c-bdb07581ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons = ak.to_backend(\n",
    "    ak.zip({\n",
    "        \"pt\": events_pt,\n",
    "        \"eta\": events_eta,\n",
    "        \"phi\": events_phi,\n",
    "        \"charge\": events_charge,\n",
    "    },\n",
    "    with_name=\"Momentum4D\",\n",
    "), \"cuda\")\n",
    "\n",
    "electrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada706a-dea6-43ee-b82d-ee3743e80aaf",
   "metadata": {},
   "source": [
    "With this backend, all mathematical computations are performed by CuPy (directly or through JIT compiled functions).\n",
    "\n",
    "Below is a calculation of the mass of the heaviest dielectron in each event, using\n",
    "\n",
    "$$\\sqrt{2\\,{p_T}_1\\,{p_T}_2\\left(\\cosh(\\eta_1 - \\eta_2) - \\cos(\\phi_1 - \\phi_2)\\right)}$$\n",
    "\n",
    "for the invariant mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1ab6a-733d-4bae-8a92-57c5b98b744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1, e2 = ak.unzip(ak.combinations(electrons, 2))\n",
    "z_mass = np.sqrt(\n",
    "    2*e1.pt*e2.pt * (np.cosh(e1.eta - e2.eta) - np.cos(e1.phi - e2.phi))\n",
    ")\n",
    "np.max(z_mass, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60979df-226d-420f-a608-9baf536acafd",
   "metadata": {},
   "source": [
    "GPU-bound Awkward Arrays can also be iterated over in Numba-CUDA, which allows for imperative code. However, the output must be a non-ragged CuPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cbbb1-1900-47f9-98e1-c791eac4876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.numba.register_and_check()\n",
    "\n",
    "@nb.cuda.jit(extensions=[ak.numba.cuda])\n",
    "def mass_of_heaviest_dielectron(electrons, out):\n",
    "    thread_idx = nb.cuda.grid(1)\n",
    "    if thread_idx < len(electrons):\n",
    "        electrons_in_one_event = electrons[thread_idx]\n",
    "        for i, e1 in enumerate(electrons_in_one_event):\n",
    "            for e2 in electrons_in_one_event[i + 1:]:\n",
    "                if e1.charge != e2.charge:\n",
    "                    m = math.sqrt(\n",
    "                        2*e1.pt*e2.pt * (math.cosh(e1.eta - e2.eta) - math.cos(e1.phi - e2.phi))\n",
    "                    )\n",
    "                    if m > out[thread_idx]:\n",
    "                        out[thread_idx] = m\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(len(electrons) / 1024))\n",
    "\n",
    "out = cp.zeros(len(electrons), dtype=np.float32)\n",
    "mass_of_heaviest_dielectron[blocks_per_grid, threads_per_block](electrons, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5792b91-638d-43a7-969d-a74a74776a91",
   "metadata": {},
   "source": [
    "To make this a little more readable, let's rewrite it as a `__device__` function, which is a function that has a return value (doesn't have to overwrite an array) but can only be called from GPU-bound functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff2d11-0759-452c-9b7b-9512f99760da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.cuda.jit(extensions=[ak.numba.cuda], device=True)\n",
    "def compute_mass(event):\n",
    "    out = np.float32(0)\n",
    "    for i, e1 in enumerate(event):\n",
    "        for e2 in event[i + 1:]:\n",
    "            if e1.charge != e2.charge:\n",
    "                m = math.sqrt(\n",
    "                    2*e1.pt*e2.pt * (math.cosh(e1.eta - e2.eta) - math.cos(e1.phi - e2.phi))\n",
    "                )\n",
    "                if m > out:\n",
    "                    out = m\n",
    "    return out\n",
    "\n",
    "@nb.cuda.jit(extensions=[ak.numba.cuda])\n",
    "def mass_of_heaviest_dielectron_2(events, out):\n",
    "    thread_idx = nb.cuda.grid(1)\n",
    "    if thread_idx < len(events):\n",
    "        out[thread_idx] = compute_mass(events[thread_idx])\n",
    "\n",
    "# same threads_per_block, blocks_per_grid\n",
    "\n",
    "out = cp.zeros(len(electrons), dtype=np.float32)\n",
    "mass_of_heaviest_dielectron_2[blocks_per_grid, threads_per_block](electrons, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e6493-d6fe-49e4-b8b0-9f9928dcf457",
   "metadata": {},
   "source": [
    "## Lesson 5 project: Histograms and Monte Carlo on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2ea66-a616-4b20-a0e5-75aac39c90e5",
   "metadata": {},
   "source": [
    "Open `project.ipynb`, then follow its instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a82a1b-b3e5-4e7a-8427-ec752ebaf070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
